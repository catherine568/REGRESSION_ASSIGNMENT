{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqG2pWLWQ_td"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        " - Simple linear regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables:\n",
        "\n",
        "One variable, denoted x, is regarded as the predictor, explanatory, or independent variable.\n",
        "The other variable, denoted y, is regarded as the response, outcome, or dependent variable.\n",
        "Because the other terms are used less frequently today, we'll use the \"predictor\" and \"response\" terms to refer to the variables encountered in this course. The other terms are mentioned only to make you aware of them should you encounter them. Simple linear regression gets its adjective \"simple,\" because it concerns the study of only one predictor variable. In contrast, multiple linear regression, which we study later in this course, gets its adjective \"multiple,\" because it concerns the study of two or more predictor variables.\n",
        "\n",
        "2.What are the key assumptions of Simple Linear Regression?\n",
        " - There is a linear relationship between the predictors (x) and the outcome (y)\n",
        "- Predictors (x) are independent and observed with negligible error\n",
        "- Residual Errors have a mean value of zero\n",
        "- Residual Errors have constant variance\n",
        "- Residual Errors are independent from each other and predictors (x)\n",
        "\n",
        "3.What does the coefficient m represent in the equation Y=mX+c?\n",
        " - the equation of the line y = mx + c, where m is the slope of the line, and c is the y-intercept of the line. This line cuts the y-axis at the point (0, c) which is at a distance of c units from the origin."
      ],
      "metadata": {
        "id": "ILJye9P1RKVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What does the intercept c represent in the equation Y=mX+c?\n",
        " - The general equation of a straight line is y = mx + c, where m is the gradient, and y = c is the value where the line cuts the y-axis. This number c is called the intercept on the y-axis. The equation of a straight line with gradient m and intercept c on the y-axis is y = mx + c.\n",
        "\n",
        "5.How do we calculate the slope m in Simple Linear Regression?\n",
        " - The formula to calculate the slope (m) of the best fit line in linear regression is obtained by dividing the sum of the products of the differences between the observed values and their means by the sum of the squared differences between the independent variable values and their mean.\n",
        "\n",
        "6.What is the purpose of the least squares method in Simple Linear Regression?\n",
        " - Least squares is a method to apply linear regression. It helps us predict results based on an existing set of data as well as clear anomalies in our data. Anomalies are values that are too good, or bad, to be true or that represent rare cases.\n",
        "\n",
        "7.How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        " - The coefficient of determination (R²) is a number between 0 and 1 that measures how well a statistical model predicts an outcome. You can interpret the R² as the proportion of variation in the dependent variable that is predicted by the statistical model.\n",
        "\n",
        "8.What is Multiple Linear Regression?\n",
        " - Multiple linear regression is a regression model that estimates the relationship between a quantitative dependent variable and two or more independent variables using a straight line.\n",
        "\n",
        "9.What is the main difference between Simple and Multiple Linear Regression?\n",
        " - The main difference between simple and multiple regression is that multiple regression includes two or more independent variables – sometimes called predictor variables – in the model, rather than just one.\n",
        "\n",
        "10.What are the key assumptions of Multiple Linear Regression?\n",
        " - Multiple linear regression analysis is based on several fundamental assumptions that ensure the validity and reliability of its results. Understanding and checking these assumptions is crucial for accurate model interpretation and prediction.\n",
        " 1. Linear Relationship\n",
        " 2.Multivariate Normality\n",
        " 3.No Multicollinearity\n",
        " 4.Variance Inflation Factor (VIF),\n",
        " 5.Homoscedasticity\n",
        " 6.Sample Size and Variable Types\n",
        " 7.Correlation matrices\n",
        "\n",
        "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        " - Heteroskedasticity is usually defined as some variation of the phrase “non-constant error variance”, or the idea that, once the predictors have been included in the regression model, the remaining residual variability changes as a function of something that is not in the model (Cohen, West, & Aiken, 2007; Field, 2009; ...\n",
        "\n",
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        " - To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information.\n",
        "\n",
        "13.What are some common techniques for transforming categorical variables for use in regression models?\n",
        " - One-Hot Encoding. One-hot encoding, also known as dummy encoding, is a popular technique for converting categorical data into a numerical format. ...\n",
        " - Dummy Encoding. ...\n",
        "- Effect Encoding. ...\n",
        "- Label Encoding. ...\n",
        "- Ordinal Encoding. ...\n",
        "- Count Encoding. ...\n",
        "- Binary Encoding.\n",
        "\n",
        "14.What is the role of interaction terms in Multiple Linear Regression?\n",
        " - Interaction Terms\n",
        "This allows the slope coefficient for one variable to vary depending on the value of the other variable. For example, this scatter plot shows happiness level on the y-axis against stress level on the x-axis.\n",
        "\n",
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        " - A multiple regression formula has multiple slopes (one for each variable) and one y-intercept. It is interpreted the same as a simple linear regression formula—except there are multiple variables that all impact the slope of the relationship.\n",
        "\n",
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        " - The slope and the intercept define the linear relationship between two variables, and can be used to estimate an average rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of change.\n",
        "\n",
        "17.How does the intercept in a regression model provide context for the relationship between variables?\n",
        " - The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero. In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero.\n",
        "\n",
        "18.What are the limitations of using R² as a sole measure of model performance?\n",
        " - Limitations of R-Squared\n",
        "However, it doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-squared isn't necessarily good or bad—it doesn't convey the reliability of the model or whether you've chosen the right regression.\n",
        "\n",
        "19.How would you interpret a large standard error for a regression coefficient?\n",
        " -The standard error of the coefficient is always positive. Use the standard error of the coefficient to measure the precision of the estimate of the coefficient. The smaller the standard error, the more precise the estimate. Dividing the coefficient by its standard error calculates a t-value.\n",
        "\n",
        "20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        " - These plots show residuals (errors) against fitted values. In a well-behaved model, residuals are randomly scattered. But with heteroscedasticity, you'll often see patterns like a cone or fan shape—indicating that the variance of the errors increases or decreases with the predicted values.\n",
        "\n",
        "21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        " - While r-squared measures the proportion of variance in the dependent variable explained by the independent variables, it always increases when more predictors are added. Adjusted r-squared adjusts for the number of predictors and decreases if the additional variables do not contribute to the model's significance.\n",
        "\n",
        "22.Why is it important to scale variables in Multiple Linear Regression?\n",
        " - Scaling variables\n",
        "In a multiple linear regression model is an important preprocessing step to ensure that the model performs well and that the coefficients of the independent variables (predictors) are interpretable.\n",
        "\n",
        "23.What is polynomial regression?\n",
        " -  polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data. Thus, polynomial regression is a special case of linear regression.\n",
        "\n",
        "The explanatory (independent) variables resulting from the polynomial expansion of the \"baseline\" variables are known as higher-degree terms. Such variables are also used in classification settings.\n",
        "\n",
        "24.How does polynomial regression differ from linear regression?\n",
        " - Linear regression has high bias because it's a simple model, but it also has low variance, meaning it's stable across different datasets. Polynomial regression, especially with higher degrees, has low bias but high variance.\n",
        "\n",
        "25.When is polynomial regression used?\n",
        " - 3.2 Polynomial regression. Linear regression is used to establish linear regression between the variables. However, if the distribution of the data is nonlinear, polynomial regression can be used.\n",
        "\n",
        "26.What is the general equation for polynomial regression?\n",
        " - We have just implemented polynomial regression - as easy as that! In general, polynomial models are of the form y=f(x)=β0+β1x+β2x2+β3x3+… +βdxd+ + β d x d + ϵ , where d is called the degree of the polynomial.\n",
        "\n",
        "27.Can polynomial regression be applied to multiple variables?\n",
        " -  Multivariate polynomial regression is an extension of linear regression that allows for multiple input variables and non-linear relationships between the input variables and the target variable.\n",
        "\n",
        "28.What are the limitations of polynomial regression?\n",
        " - Limitations of Polynomial Regression\n",
        "1. Overfitting: Higher-degree polynomial models are susceptible to overfitting, where the model fits the training data too closely and loses generalization ability. Careful model selection and regularization techniques are required to mitigate this risk.\n",
        "2. Computational Complexity: As the degree of the polynomial increases, so does the computational complexity. Extremely high-degree polynomials can be computationally expensive and may not provide meaningful improvements in model performance.\n",
        "3. Data Requirements: Polynomial regression works best when you have a sufficient amount of data, especially for high-degree polynomials. Small datasets may not provide enough information to estimate the complex polynomial terms reliably.  \n",
        "\n",
        "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        " - The solution is to have a separate validation set on which we can evaluate the model for each polynomial degree. If we fit the model using the original data (training set) but evaluate it using the validation set, the model will overfit to the noise in the training set but not in the validation set.\n",
        "\n",
        "30.Why is visualization important in polynomial regression?\n",
        " - The visualization based on Flash – an advanced multimedia authoring and development environment for creating animations and interactive applications inside Web pages – will present a model for better understanding of curve fitting.\n",
        "\n",
        "31.How is polynomial regression implemented in Python?\n",
        " - Polynomial Regression implementations using Python\n",
        "To get the Dataset used for the analysis of Polynomial Regression, click here. Import the important libraries and the dataset we are using to perform Polynomial Regression.\n",
        "\n",
        "Python libraries make it very easy for us to handle the data and perform typical and complex tasks with a single line of code.\n",
        "\n",
        "- Pandas – This library helps to load the data frame in a 2D array format and has multiple functions to perform analysis tasks in one go.\n",
        "- Numpy – Numpy arrays are very fast and can perform large computations in a very short time.\n",
        "- Matplotlib/Seaborn – This library is used to draw visualizations.\n",
        "- Sklearn – This module contains multiple libraries having pre-implemented functions to perform tasks from data preprocessing to model development and evaluation.   \n",
        "\n",
        "       "
      ],
      "metadata": {
        "id": "RM-DHLAgTuqq"
      }
    }
  ]
}